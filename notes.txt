[√] Put some mock tags in there of different types
    [ ] Create a tag object
    [ ] Save some in the database
    [ ] Be able to make a query
[ ] process to index / map-reduce / whatever / based on the keywords 
[ ] Create an API, and HTML interface, that return the tags for a given search term 
[ ] Figure out how to add different weights to different services that we know are better?

[ ] Mock Tags in indexed form (map reduce + id?) (yes, eventually, but for now, just the docs)
[√] Function that returns docs matching the search term
[ ] API returning docs that match the search term in JSON
[ ] API returning docs that match the search term in HTML
[ ] Ingest Aol, CBS, and Wikipedia


THE REAL STEPS
[√] Get some mock tags
[√] Method 1: Return anything matching the search term - in any order
[√] API: Make a web route returning a JSON representation
    [√] Get web running
    [√] Get json output with dummy data
    [√] Get json running with bson data
    [√] Get real data / route in there

[√] Rob: what do the endpoints look like?
    - 1. Get entities
    - 2. Hit up each endpoint for information about it. 
    - 3. Reduce into a single document (so sense of importance or order)

[√] API: Make a web route returning HTML representation
    http://stackoverflow.com/questions/5770168/templating-packages-for-haskell


LEFTOVERS
[ ] I want to see the score of manual sorting

[√] API 2: Manually add a search term and a score to each document
    [√] Think about which results are strongest for a given search term
    [√] Make it return those with the strongest first
    [ ] Make it work for "Lady" and "Lady Gaga"
    [ ] Lady: should return wikipedia for Lady
    [ ] Lady Gaga: should return results better

[ ] API 3: Source weighting. Figure out how to favor Aol over Wikipedia, in spite of its score
    [ ] whichever one doesn't come up first, make that one come up first. 

[ ] API 4: Breifs: Add a "Brief" in there, and give it maximum score

[ ] Enhancement: Make a Tag type and serialize that instead

[ ] API 5: Extract Search Terms + Assign Score
    [ ] Have them show up for different key word searches

    [ ] How are you going to do this at all?
        - http://blog.tupil.com/stemming-with-haskell/ - stemmiing isn't really what you want

    [ ] Start with a giant list of keywords, look for them in the title or text, then compare distance
    
    [ ] Make it lazy, so you can start with a keyword, search for that keyword, analyze all documents containing said keyword, and write them to the search results collection. Then hit the normal query

    [ ] http://stackoverflow.com/questions/1575246/how-do-i-extract-keywords-used-in-text
        [ ] Go through the body of each article and determine keywords that way.
        [ ] Then determine score based on the "Purity" of each term. So if it has "Lady Gaga" 20, 
            we give that the highest normalized score, and it gets a very low score for other things. 

    [ ] Consider extracting keywords individually, and using a single query to make sure it has both, then sorting by the combined score (somehow)

    [ ] http://portal.acm.org/citation.cfm?id=1072370 - read. From IBM
    
    [ ] http://en.wikipedia.org/wiki/Terminology_extraction

    [ ] Packages! http://hackage.haskell.org/packages/archive/pkg-list.html#cat:natural%20language%20processing

    [ ] Use full-text search (can I manipulate the results?) There are sphinx packages
    
[ ] API 6: Consider all search terms to be a single document. Just return the document of results. 

[ ] API 7: Create full text search by:
    [ ] Stemming the tags, recording all keywords
    [ ] Stemming the queries, and making sure the doc has all the stems

