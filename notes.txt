[√] Put some mock tags in there of different types
    [ ] Create a tag object
    [ ] Save some in the database
    [ ] Be able to make a query
[ ] process to index / map-reduce / whatever / based on the keywords 
[ ] Create an API, and HTML interface, that return the tags for a given search term 
[ ] Figure out how to add different weights to different services that we know are better?

[ ] Mock Tags in indexed form (map reduce + id?) (yes, eventually, but for now, just the docs)
[√] Function that returns docs matching the search term
[ ] API returning docs that match the search term in JSON
[ ] API returning docs that match the search term in HTML
[ ] Ingest Aol, CBS, and Wikipedia


THE REAL STEPS
[√] Get some mock tags
[√] Method 1: Return anything matching the search term - in any order
[√] API: Make a web route returning a JSON representation
    [√] Get web running
    [√] Get json output with dummy data
    [√] Get json running with bson data
    [√] Get real data / route in there

[√] Rob: what do the endpoints look like?
    - 1. Get entities
    - 2. Hit up each endpoint for information about it. 
    - 3. Reduce into a single document (so sense of importance or order)

[√] API: Make a web route returning HTML representation
    http://stackoverflow.com/questions/5770168/templating-packages-for-haskell


LEFTOVERS
[√] I want to see the score of manual sorting --- Just change tags to have a score and term.. 
    -- then add them per a process you can tweakify
    -- it doesn't hurt to add them more than once

[√] Enhancement: Make a Tag type and serialize that instead


RESEARCH
[√] Easy to do a two-step query, if we need to
[ ] Can we have terms: ["a","b","c"] and find all docs that have both a AND c?





[√] API 2: Manually add a search term and a score to each document
    [√] Think about which results are strongest for a given search term
    [√] Make it return those with the strongest first
    [√] Make it work for "Lady" and "Lady Gaga"
    [√] Lady: should return wikipedia for Lady
    [√] Lady Gaga: should return results better

[√] API 3: Source weighting. Figure out how to favor Aol over Wikipedia, in spite of its score
    [√] whichever one doesn't come up first, make that one come up first. 
    [√] term score, source score. 
    [√] Automatically add it. get a systam going

    [ ] Good way to weight it mathematically. 
        - need better term scores
        - then don't let them take over. Right now it's too much
        - Naw, I need REAL term scores

[√] API 4: Breifs: Add a "Brief" in there, and give it maximum score

[ ] API 5: Term scores!
    [√] Manually term score on input
    [√] Term Score them client-side, after getting a match at all
    [√] Lazily extract them from docs that match at all. Term Query -> Get Docs -> Score Docs -> Re-run query
    [√] Add source weighting as well
    [ ] Try assigning a score to each word individually, and combine them somehow

[ ] API 6: Consider all search terms to be a single document. Just return the document of results. 
    [ ] You'd have to take the result of a search and combine it no?

[ ] API 7: Create full text search by:
    [ ] Stemming the tags, recording all keywords
    [ ] Stemming the queries, and making sure the doc has all the stems

[ ] Real Data Sources - try parsing Aol / CBS. Can it be done quickly, perhaps?











KEYWORD IDEAS
    [ ] Have them show up for different key word searches

    [ ] How are you going to do this at all?
        - http://blog.tupil.com/stemming-with-haskell/ - stemmiing isn't really what you want

    [ ] Start with a giant list of keywords, look for them in the title or text, then compare distance
    
    [ ] Make it lazy, so you can start with a keyword, search for that keyword, analyze all documents containing said keyword, and write them to the search results collection. Then hit the normal query

    [ ] http://stackoverflow.com/questions/1575246/how-do-i-extract-keywords-used-in-text
        [ ] Go through the body of each article and determine keywords that way.
        [ ] Then determine score based on the "Purity" of each term. So if it has "Lady Gaga" 20, 
            we give that the highest normalized score, and it gets a very low score for other things. 

    [ ] Consider extracting keywords individually, and using a single query to make sure it has both, then sorting by the combined score (somehow)

    [ ] http://portal.acm.org/citation.cfm?id=1072370 - read. From IBM
    
    [ ] http://en.wikipedia.org/wiki/Terminology_extraction

    [ ] Packages! http://hackage.haskell.org/packages/archive/pkg-list.html#cat:natural%20language%20processing

    [ ] Use full-text search (can I manipulate the results?) There are sphinx packages
    
